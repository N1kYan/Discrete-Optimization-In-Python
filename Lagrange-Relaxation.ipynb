{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method explained in the following tries to solve our problem of finding an integral solution from a somewhat different perspective.\n",
    "\n",
    "# Lagrange Relaxation\n",
    "\n",
    "Consider the MIP\n",
    "<br><br>\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\begin{array}{ll@{}ll}\n",
    "    \\displaystyle \\text{max} & c^T x &\\\\\n",
    "    \\displaystyle \\text{s.t.}& A^1 x  & \\leq b^1 &\\\\\n",
    "    \\displaystyle            & A^2 x  & \\leq b^2 &\\\\\n",
    "                             &     x  & \\in Z^p \\times R^{n-p}\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$\n",
    "<br><br>\n",
    "The idea here is, that we have already split-up our constraints and put the \"easier\" part into $P(A^2,b^2)$, while the harder part is in $P(A^1, b^1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can replace the \"hard\" part of the constraints by a penalty in the objective function, yielding the so-called <b>Lagrange Equation</b>:\n",
    "<br><br>\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\begin{array}{ll@{}ll}\n",
    "    \\displaystyle L(\\lambda) & = \\text{max}\\{c^Tx + \\lambda^T(b^1 - A^1 x)|x\\in X^2\\} &\\\\\n",
    "    \\displaystyle \\text{with} & \\lambda \\in R^{m_1}_{\\geq 0}, &\\\\\n",
    "    & X^2 := \\{x|A^2x\\leq b^2\\} \\cap (Z^p \\times R^{n-p})\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>Lagrange Relaxation</b> now tries to minimize this equation, which gives an upper bound on the optimal objective of the MIP $\\forall \\lambda \\geq 0$:\n",
    "<br><br>\n",
    "$\\text{min}_{\\lambda \\geq 0} L(\\lambda) = \\text{max}\\{c^Tx|A^1x \\leq b^1, x \\in \\text{conv}(X^2)\\}$\n",
    "<br><br>\n",
    "This yields an approximation at least as close to the true optimum of the MIP as the optimum of the LP relaxation:\n",
    "<br><br>\n",
    "$z_{MIP} \\leq \\text{min}_{\\lambda \\geq 0} L(\\lambda) \\leq z_{LP}$\n",
    "<br><br>\n",
    "Note that $L(\\lambda)$ is convex, so we can optimize it using the <b>subgradient method</b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subgradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f:R^r\\rightarrow R$ be a convex function. Then, $h\\in R^r$ is called a <b>subgradient</b> of $f$ at $\\lambda$, if\n",
    "<br><br>\n",
    "$f(\\lambda')\\geq f(\\lambda) + h^T(\\lambda' - \\lambda) \\>\\>\\>\\> \\forall \\lambda' \\in R^r$\n",
    "<br><br>\n",
    "Note that if the function $f$ is minimzed at $\\lambda^*$, then\n",
    "<br><br>\n",
    "$0 \\geq f(\\lambda^*) - f(\\lambda)\\geq h^T(\\lambda^* - \\lambda)$\n",
    "<br><br>\n",
    "Using this subgradient in a gradient-descent type algorithm ($-h$ points towards $\\lambda^*$) yields a linear over-estimation of the objective, which allows to also optimze non-smooth functions (which is the case for our function $L(\\lambda)$, as it is the maximum of finitely many affince functions$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three important questions to finally run this approach remain.\n",
    "1. <i>How do we know when to stop the procedure?</i>\n",
    "<br><br>\n",
    "The answer here, to keep it simple, is to stop at a certain threshold to the distance to the optimum $f(\\lambda^*) - f(\\lambda)$\n",
    "<br><br>\n",
    "2. <i>How do we calculate the subgradient?</i>\n",
    "<br><br>\n",
    "This has to be answered specifically for the problem we are looking at. For the lagrange relaxation one can show that if $x^\\lambda$ is an optimal solution of $L(\\lambda)$, then $h=b^1-A^1x\\lambda$ is a subgradient of $L$ at $\\lambda$.\n",
    "<br><br>\n",
    "3. <i>How can we choose the step-size appropriately?</i>\n",
    "<br><br>\n",
    "For this approach, it can be shown that if $f$ attains a minimum and there is a bound $H\\in R:||h^k||\\leq H \\>\\> \\forall k$ for the norm of subgradients $h^k$ used in every step $k$ of the method, we can choose the stepsize according to \n",
    "<br><br>\n",
    "$\\displaystyle \\sum_{k=0}^\\infty \\mu_k = \\infty, \\sum_{k=0}^\\infty \\mu_k^2 < \\infty$\n",
    "<br><br>\n",
    "then the row of produces optima $(\\bar f_k)_k$ converges to the minimum $f^*=f(\\lambda^*)$. If we choose $\\mu_k = \\frac{1}{k}$, then even $(\\lambda^k)_k$ converges to $\\lambda^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this method in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world°°°!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"hello world°°°!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
